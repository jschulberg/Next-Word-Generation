{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(bayes-inference)=\n",
    "# Bayesian Inference\n",
    "In this section, we'll use Bayesian inference for the task of [*next word prediction*](https://arxiv.org/abs/2306.17184).\n",
    "\n",
    "## Next Word Prediction\n",
    "A common task that language models try to solve is next word prediction. Given a series of words in a sentence, can the language model predict the *next* word that should appear? Ideally, the model would use understanding of the prior words that appeared \n",
    "\n",
    "\n",
    "\n",
    "## Bayes' Formula\n",
    "The traditional Bayes' formula, given events A and B, is:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) * P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Given a prior, $\\theta$, which represents the probability distribution of a data object *before* it's observed, and a likelihood $y$, which represents the probability of falling under a specific category/class, we can generalize the above formula to compute the posterior as such:\n",
    "\n",
    "$$\n",
    "p(\\theta|y) = \\frac{p(y|\\theta)*p(\\theta)}{p(y)}\n",
    "$$\n",
    "\n",
    "where $p(\\theta|y)$ represents the posterior probability: the probability after the evidence from our prior is considered.\n",
    "\n",
    "\n",
    "## A Simple Application of Bayes for Next Word Prediction\n",
    "Let's start simple. We'll take some text and tokenize it into words. I'll then create a matrix that shows the probability that a word appears given the word that comes directly before it. Again, this is a naive approach because I will *only* look at the word that comes immediately before. That makes the approach act more like a [Markovian transition matrix](https://www.jstor.org/stable/2584334), since it's just looking at the state immediately before to make a prediction, rather than all the events that led up to the prediction.\n",
    "\n",
    "![](images/markov.png)\n",
    "\n",
    "Markov chain analysis is a mathematical framework used to model and analyze systems that undergo transitions between different states over time. A Markov chain is a stochastic process where the future state depends only on the current state and not on the sequence of events that preceded it. In this case our \"state\" is the current word being observed. We will use the current state (word) to predict the next state (word).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I am so excited to graduate from Georgia Tech! \n",
      "I have loved my time studying at Georgia Tech and cannot wait to graduate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "I am so excited to graduate from Georgia Tech! \n",
    "I have loved my time studying at Georgia Tech and cannot wait to graduate.\n",
    "'''\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Bi-gram Co-occurrence Matrix\n",
    "To start, I'll tokenize the text into words and split it into bigrams. Using the bigrams, I'll create a co-occurence matrix, $P_{ij}$ where $i$ represents the $i^{\\text{th}}$ word and $j$ represents the $j^{\\text{th}}$ word. The value of $P_{ij}$ then is the probability that word $j$ occurs immediately after word $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>am</th>\n",
       "      <th>so</th>\n",
       "      <th>excited</th>\n",
       "      <th>to</th>\n",
       "      <th>graduate</th>\n",
       "      <th>from</th>\n",
       "      <th>georgia</th>\n",
       "      <th>tech</th>\n",
       "      <th>have</th>\n",
       "      <th>loved</th>\n",
       "      <th>my</th>\n",
       "      <th>time</th>\n",
       "      <th>studying</th>\n",
       "      <th>at</th>\n",
       "      <th>and</th>\n",
       "      <th>can</th>\n",
       "      <th>not</th>\n",
       "      <th>wait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excited</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graduate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>georgia</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loved</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>studying</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wait</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            i   am   so  excited   to  graduate  from  georgia  tech  have  \\\n",
       "i         0.0  0.5  0.0      0.0  0.0       0.0   0.0      0.0   0.0   0.5   \n",
       "am        0.0  0.0  1.0      0.0  0.0       0.0   0.0      0.0   0.0   0.0   \n",
       "so        0.0  0.0  0.0      1.0  0.0       0.0   0.0      0.0   0.0   0.0   \n",
       "excited   0.0  0.0  0.0      0.0  1.0       0.0   0.0      0.0   0.0   0.0   \n",
       "to        0.0  0.0  0.0      0.0  0.0       1.0   0.0      0.0   0.0   0.0   \n",
       "graduate  0.0  0.0  0.0      0.0  0.0       0.0   1.0      0.0   0.0   0.0   \n",
       "from      0.0  0.0  0.0      0.0  0.0       0.0   0.0      1.0   0.0   0.0   \n",
       "georgia   0.0  0.0  0.0      0.0  0.0       0.0   0.0      0.0   1.0   0.0   \n",
       "tech      0.5  0.0  0.0      0.0  0.0       0.0   0.0      0.0   0.0   0.0   \n",
       "have      0.0  0.0  0.0      0.0  0.0       0.0   0.0      0.0   0.0   0.0   \n",
       "loved     0.0  0.0  0.0      0.0  0.0       0.0   0.0      0.0   0.0   0.0   \n",
       "my        0.0  0.0  0.0      0.0  0.0       0.0   0.0      0.0   0.0   0.0   \n",
       "time      0.0  0.0  0.0      0.0  0.0       0.0   0.0      0.0   0.0   0.0   \n",
       "studying  0.0  0.0  0.0      0.0  0.0       0.0   0.0      0.0   0.0   0.0   \n",
       "at        0.0  0.0  0.0      0.0  0.0       0.0   0.0      1.0   0.0   0.0   \n",
       "and       0.0  0.0  0.0      0.0  0.0       0.0   0.0      0.0   0.0   0.0   \n",
       "can       0.0  0.0  0.0      0.0  0.0       0.0   0.0      0.0   0.0   0.0   \n",
       "not       0.0  0.0  0.0      0.0  0.0       0.0   0.0      0.0   0.0   0.0   \n",
       "wait      0.0  0.0  0.0      0.0  1.0       0.0   0.0      0.0   0.0   0.0   \n",
       "\n",
       "          loved   my  time  studying   at  and  can  not  wait  \n",
       "i           0.0  0.0   0.0       0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "am          0.0  0.0   0.0       0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "so          0.0  0.0   0.0       0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "excited     0.0  0.0   0.0       0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "to          0.0  0.0   0.0       0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "graduate    0.0  0.0   0.0       0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "from        0.0  0.0   0.0       0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "georgia     0.0  0.0   0.0       0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "tech        0.0  0.0   0.0       0.0  0.0  0.5  0.0  0.0   0.0  \n",
       "have        1.0  0.0   0.0       0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "loved       0.0  1.0   0.0       0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "my          0.0  0.0   1.0       0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "time        0.0  0.0   0.0       1.0  0.0  0.0  0.0  0.0   0.0  \n",
       "studying    0.0  0.0   0.0       0.0  1.0  0.0  0.0  0.0   0.0  \n",
       "at          0.0  0.0   0.0       0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "and         0.0  0.0   0.0       0.0  0.0  0.0  1.0  0.0   0.0  \n",
       "can         0.0  0.0   0.0       0.0  0.0  0.0  0.0  1.0   0.0  \n",
       "not         0.0  0.0   0.0       0.0  0.0  0.0  0.0  0.0   1.0  \n",
       "wait        0.0  0.0   0.0       0.0  0.0  0.0  0.0  0.0   0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, Math, Latex\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def create_markov_matrix(text, order = 1):\n",
    "    # Get a list of the tokenized words without punctuation\n",
    "    tokens = [word.lower() for word in nltk.word_tokenize(text) if word not in string.punctuation]\n",
    "    unique_tokens = list(dict.fromkeys(tokens))\n",
    "    # print(unique_tokens)\n",
    "\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "    # print(bigrams)\n",
    "\n",
    "    # Create a DataFrame where the rows are words and the columns are words\n",
    "    df = pd.DataFrame(0, columns=unique_tokens, index=unique_tokens)\n",
    "\n",
    "    # Loop through each of the bigrams (tuples), locate them in the DF, and add 1\n",
    "    for i in bigrams:\n",
    "        df.loc[i[0],i[1]] += 1\n",
    "\n",
    "    # Convert the DataFrame from raw word counts to probabilities\n",
    "    w_normalized = normalize(df, norm='l1', axis=1)\n",
    "\n",
    "    if order > 1:\n",
    "        w_normalized = np.linalg.matrix_power(w_normalized, order)\n",
    "        \n",
    "    df_normalized = pd.DataFrame(w_normalized, columns=unique_tokens, index=unique_tokens)\n",
    "\n",
    "    return df_normalized\n",
    "\n",
    "df_normalized = create_markov_matrix(text, order = 1)\n",
    "\n",
    "display(df_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Markov transition matrix above we can see how, in the first row, given the word `I` (our current state), there are two words that could appear (our next state):\n",
    "\n",
    "$$\n",
    "P(\\text{am } | \\text{ I}) = .5 \\\\\n",
    "P(\\text{have } | \\text{ I}) = .5\n",
    "$$\n",
    "\n",
    "It's important to note that these probabilities will **always** add up to 1; an important property of a [*row stochastic*](https://acme.byu.edu/00000179-af25-d5e1-a97b-bf6512fd0000/markov2020-pdf#:~:text=Thus%2C%20each%20of%20the%20columns,transition%20matrix%20sum%20to%201.&text=A%20transition%20matrix%20where%20the,stochastic%20(or%20left%20stochastic).) Markov chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected `am` as the next word to follow `I` with probability 0.500.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'am'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reshape_mm(mat):\n",
    "    df_reshaped = mat.reset_index() \\\n",
    "                .rename(columns = {'index': 'first_word'}) \\\n",
    "                .melt(id_vars = 'first_word',\n",
    "                    var_name = 'second_word',\n",
    "                    value_name = 'p') \\\n",
    "                .sort_values('first_word')\n",
    "\n",
    "    # Only keep actual words\n",
    "    df_reshaped = df_reshaped[df_reshaped['first_word'].apply(lambda word: word.isalpha())]\n",
    "    df_reshaped = df_reshaped[df_reshaped['second_word'].apply(lambda word: word.isalpha())]\n",
    "\n",
    "    return df_reshaped.loc[(df_reshaped['p'] > 0)]\n",
    "\n",
    "\n",
    "def find_next_word(mat, word, print_ = True):\n",
    "    df_reshaped = reshape_mm(mat)\n",
    "\n",
    "    # Filter our dataframe to the word of interest\n",
    "    # Also make sure the second word is actually a word and not some punctuation that snuck in\n",
    "    df_word = df_reshaped.loc[(df_reshaped['first_word'].str.lower() == word.lower()) & (df_reshaped['second_word'].str.isalpha())]\n",
    "\n",
    "    # Sample using the probability column\n",
    "    next_word_df = df_word.sample(1, weights = 'p')\n",
    "    next_word = next_word_df['second_word'].iloc[0]\n",
    "    p = next_word_df['p'].iloc[0]\n",
    "\n",
    "\n",
    "    if print_:\n",
    "        print(f'Selected `{next_word}` as the next word to follow `{word}` with probability {p:.3f}.')\n",
    "\n",
    "    return next_word\n",
    "\n",
    "find_next_word(df_normalized, word = 'I')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broader Example\n",
    "Let's apply this training mechanism to help write something even longer. For example, I'll load in the Georgia Tech student code of conduct and see if I can generate a new sentence based on it. I'll show a few sentence examples to better understand how the algorithm probabilistically builds sentences -- so that way each time it consults the Markov transition matrix, it selects a new word. I'll also try to define stop words so the sentence doesn't end with a stop word and flows a bit more naturally.\n",
    "\n",
    "### Training Data\n",
    "Our training data in this example is the text of interest. Using the training data, we'll build a Markov transition matrix. It's important to note that the more training data we have, the better our next word prediction will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEORGIA TECH HONOR CHALLENGE STATEMENT\n",
      "\n",
      "I commit to uphold the ideals of honor and integrity by refu ...\n",
      "The probability that we see the word `Tech` given that the word `Georgia` appears, based on our training dataset and Markov Transition matrix is 0.703\n"
     ]
    }
   ],
   "source": [
    "new_text = open('honor_code.txt', 'r').read()\n",
    "\n",
    "print(new_text[:100], '...')\n",
    "\n",
    "# Build our Markov transition matrix\n",
    "mm = create_markov_matrix(new_text)\n",
    "\n",
    "print(f\"The probability that we see the word `Tech` given that the word `Georgia` appears, based on our training dataset and Markov Transition matrix is {mm.loc['georgia', 'tech']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've also included in the text above what should be a very common phrase -- Georgia Tech -- to show $P(\\text{Tech } | \\text{ Georgia}) = .703$. That is, given the word `Georgia` appears, there's a 70.3% chance that the word `Tech` appears directly after, regardless of any words that come before `Georgia`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Students according institute becomes aware academic credit notations.\n",
      "Notations indicating rules conduct panel respondent drugs version.\n",
      "Version allegation anyone community regarding georgia institute official.\n"
     ]
    }
   ],
   "source": [
    "n_sent = 3\n",
    "min_words = 8\n",
    "\n",
    "def build_sents(mm, n_sent = 3, min_words = 8):\n",
    "    for i, sent in enumerate(range(n_sent)):\n",
    "        if i == 0:\n",
    "            starting_word = 'students'\n",
    "        else:\n",
    "            starting_word = new_word\n",
    "\n",
    "        sentence = [starting_word.title()]\n",
    "        \n",
    "        # Create boolean conditions for our while loop\n",
    "        keep_going = True\n",
    "        idx = 0\n",
    "\n",
    "        while (keep_going) or (len(sentence) < min_words):\n",
    "        # for idx, word in enumerate(range(n_words)):\n",
    "            if idx == 0:\n",
    "                new_word = starting_word\n",
    "\n",
    "            new_word = find_next_word(mm, word = new_word, print_ = False)\n",
    "\n",
    "            sentence.append(new_word)\n",
    "\n",
    "            # If our final word is a stop word or we're over the minimum number of words\n",
    "            # stop building our sentence and get rid of the stop word\n",
    "            if new_word in stop_words:\n",
    "                keep_going = False\n",
    "                sentence.pop()\n",
    "                new_word = find_next_word(mm, word = new_word, print_ = False)\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "        print(f\"{' '.join(sentence)}.\")\n",
    "\n",
    "build_sents(mm, n_sent = 3, min_words = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we've managed to create brand new, never-before-seen sentences using a Markov transition matrix trained on the Georgia Tech honor code! While the sentences might not exactly make sense, we can begin to understand what the sentence is trying to say. But how can we actually make these human-readable? In the [next section](bayes-improvement), I'll explore various methods for improving the quality of these sentences.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('llm_fix')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10881fbc70031dd02aca03a6db676ece255e600344216b915d532128331b5ec1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
