{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(bayes-improvement)=\n",
    "# Methods for Improvement\n",
    "There are a few ways we can improve the quality of our sentences:\n",
    "\n",
    "- [Better Data Preparation](lemmatize)\n",
    "- [Increase Training Data](increase-data)\n",
    "- [Bigrams --> Trigrams](ngrams)\n",
    "- [Increase the Order of Markov Chain](increase-order)\n",
    "\n",
    "As a note, I won't actually apply all of these methods due to compute limitations, but will still explain each method in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEORGIA TECH HONOR CHALLENGE STATEMENT\n",
      "\n",
      "I commit to uphold the ideals of honor and integrity by refu ...\n",
      "\n",
      "Creating Markov matrix of order 1...\n",
      "\n",
      "mm.shape=(1542, 1542)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, Math, Latex\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def create_markov_matrix(text, order = 1):\n",
    "    print(f'\\nCreating Markov matrix of order {order}...\\n')\n",
    "    # Get a list of the tokenized words without punctuation\n",
    "    tokens = [word.lower() for word in nltk.word_tokenize(text) if word not in string.punctuation and word.isalpha()]\n",
    "    unique_tokens = list(dict.fromkeys(tokens))\n",
    "    # print(unique_tokens)\n",
    "\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "    # print(bigrams)\n",
    "\n",
    "    # Create a DataFrame where the rows are words and the columns are words\n",
    "    df = pd.DataFrame(0, columns=unique_tokens, index=unique_tokens)\n",
    "\n",
    "    # Loop through each of the bigrams (tuples), locate them in the DF, and add 1\n",
    "    for i in bigrams:\n",
    "        df.loc[i[0],i[1]] += 1\n",
    "\n",
    "    # Convert the DataFrame from raw word counts to probabilities\n",
    "    w_normalized = normalize(df, norm='l1', axis=1)\n",
    "\n",
    "    if order > 1:\n",
    "        w_normalized = np.linalg.matrix_power(w_normalized, order)\n",
    "        \n",
    "    df_normalized = pd.DataFrame(w_normalized, columns=unique_tokens, index=unique_tokens)\n",
    "\n",
    "    return df_normalized\n",
    "\n",
    "    \n",
    "text = open('honor_code.txt', 'r').read()\n",
    "\n",
    "print(text[:100], '...')\n",
    "\n",
    "# Build our Markov transition matrix\n",
    "mm = create_markov_matrix(text)\n",
    "print(f'{mm.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(lemmatize)=\n",
    "## Better Data Preparation\n",
    "There are a few ways to better prepare our data. For example, we have different variants of the same word ('student' *and* 'students'). Because we don't coalesce these similar words, they receive different probabilities in our Markov transition matrix. \n",
    "\n",
    "### Lemmatization\n",
    "Thus, we could do a better job of lemmatizing the words. Lemmatization will:\n",
    "\n",
    "- Improve the accuracy of this analysis by more heavily weighting words with high probabilities but slightly different endings;\n",
    "- Increase computation time by reducing the size of our Markov chain matrices\n",
    "\n",
    "Here are some examples of how lemmatization works:\n",
    "\n",
    "- students --> student\n",
    "- corpora --> corpus\n",
    "- better --> good\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def reshape_mm(mat):\n",
    "    df_reshaped = mat.reset_index() \\\n",
    "                .rename(columns = {'index': 'first_word'}) \\\n",
    "                .melt(id_vars = 'first_word',\n",
    "                    var_name = 'second_word',\n",
    "                    value_name = 'p') \\\n",
    "                .sort_values('first_word')\n",
    "\n",
    "    # Only keep actual words\n",
    "    df_reshaped = df_reshaped[df_reshaped['first_word'].apply(lambda word: word.isalpha())]\n",
    "    df_reshaped = df_reshaped[df_reshaped['second_word'].apply(lambda word: word.isalpha())]\n",
    "\n",
    "    return df_reshaped.loc[(df_reshaped['p'] > 0)]\n",
    "\n",
    "df_reshaped = reshape_mm(mm)\n",
    "print(f'{df_reshaped.shape=}')\n",
    "# df_reshaped.head(10).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Georgia institute required institute official outlined initiation.\n",
      "Initiation instructor finds ensure conduct investigator means.\n",
      "Means rather grounds accepted allegations case misconduct.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "n_sent = 2\n",
    "min_words = 7\n",
    "\n",
    "\n",
    "def find_next_word(mat, word, print_ = True):\n",
    "    df_reshaped = reshape_mm(mat)\n",
    "    \n",
    "    # Filter our dataframe to the word of interest\n",
    "    df_word = df_reshaped.loc[(df_reshaped['first_word'].str.lower() == word.lower())]\n",
    "\n",
    "    # Sample using the probability column\n",
    "    next_word_df = df_word.sample(1, weights = 'p', replace = True)\n",
    "    next_word = next_word_df['second_word'].iloc[0]\n",
    "    p = next_word_df['p'].iloc[0]\n",
    "\n",
    "\n",
    "    if print_:\n",
    "        print(f'Selected `{next_word}` as the next word to follow `{word}` with probability {p:.3f}.')\n",
    "        \n",
    "    return next_word\n",
    "\n",
    "def build_sents(mm, n_sent = 3, min_words = 8, starter = 'Georgia'):\n",
    "    for i, sent in enumerate(range(n_sent)):\n",
    "        if i == 0:\n",
    "            starting_word = starter\n",
    "        else:\n",
    "            starting_word = new_word\n",
    "\n",
    "        sentence = [starting_word.title()]\n",
    "        \n",
    "        # Create boolean conditions for our while loop\n",
    "        keep_going = True\n",
    "        idx = 0\n",
    "\n",
    "        while (keep_going) or (len(sentence) < min_words):\n",
    "        # for idx, word in enumerate(range(n_words)):\n",
    "            if idx == 0:\n",
    "                new_word = starting_word\n",
    "\n",
    "            new_word = find_next_word(mm, word = new_word, print_ = False)\n",
    "\n",
    "            sentence.append(new_word)\n",
    "\n",
    "            # If our final word is a stop word or we're over the minimum number of words\n",
    "            # stop building our sentence and get rid of the stop word\n",
    "            if new_word in stop_words:\n",
    "                keep_going = False\n",
    "                sentence.pop()\n",
    "                new_word = find_next_word(mm, word = new_word, print_ = False)\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "        print(f\"Sentence {i}: {' '.join(sentence)}.\")\n",
    "\n",
    "build_sents(mm, n_sent = n_sent, min_words = min_words, starter = 'Georgia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(increase-data)=\n",
    "## Increase Training Data\n",
    "The Georgia Tech honor code text that I imported is ~13k words, 1600 of which are unique tokens. This is a good start, but if we want to more accurately perform the task of next word prediction, we want to increase our training dataset **substantially**. Ideally, we use as much text related to our problem of interest as possible. \n",
    "\n",
    "### Considerations\n",
    "This raises the potential for serious issues given the way that the code has been structured. With just 13k words, I've created a 1625x1625 matrix; with a serious amount of training data, the storage needed to contain the larger matrix would be unsustainable. If I wanted to go this route, I would have to make use of some of the properties of sparse matrices, like those available in `scipy.sparse` to speed up computation.\n",
    "\n",
    "*Note: Due to compute limitations, I won't apply this here, but the effects of this approach should be evident.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ngrams)=\n",
    "### Unigrams --> n-grams\n",
    "Currently, we're essentially looking at unigrams: we've essentially word tokenized our text (e.g. 'Georgia'). It *might* improve our performance to look at n-word-pairs like bigrams (e.g. 'Georgia Tech') or trigrams (e.g. 'Georgia Tech rules'). There are even methods to make the Markov chain *simultaneously* look at unigrams, bigrams, and trigrams, as outlined by the process below:\n",
    "\n",
    "1. If the current state of text has two words prior to it, look at the Markov chain of trigrams.\n",
    "1. If the Markov chain does not detect any patterns from the trigram *or* the phrase only consists of two words, look at the training data of bigrams.\n",
    "1. If the Markov chain does not detect any patterns from the bigram *or* the phrase only consists of single words, look at the training data of unigrams.\n",
    "\n",
    "*Note: To include options like trigrams, we need to **seriously** increase the size of our training data because of how much less likely it is for trigrams to appear in context.*\n",
    "\n",
    "To quickly implement this, I'll use a package, [Markovify](https://github.com/jsvine/markovify#advanced-usage), already developed for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using 1-gram to predict the next state in the Markov chain.\n",
      "\t The Institute shall normally be adjudicated by the investigation and/or Degree: The Respondent may be a brief, written or the faculty, and Supplementary Requirements imposed if interim suspension.\n",
      "\n",
      "Using 2-gram to predict the next state in the Markov chain.\n",
      "\t In cases where the Respondent fails to complete assigned Sanctions.\n",
      "\n",
      "Using 3-gram to predict the next state in the Markov chain.\n",
      "\t All graduate students are involved in research and scholarly activities which occur outside of the classroom.\n"
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "\n",
    "max_state = 3\n",
    "num_sent = 1\n",
    "\n",
    "for state in range(max_state):\n",
    "    print(f'\\nUsing {state+1}-gram to predict the next state in the Markov chain.')\n",
    "    # Build the model.\n",
    "    text_model = markovify.Text(text, state_size = state+1)\n",
    "\n",
    "    # Print three randomly-generated sentences of no more than 280 characters\n",
    "    for i in range(num_sent):\n",
    "        print('\\t', text_model.make_short_sentence(200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3-gram results are extremely promising! It makes sense that these would make more sense because they \"force\" three-word pairs to stay together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(increase-order)=\n",
    "## Increase the Order of Markov Chain\n",
    "We have set up our approach to make use of Markov Chains' memoryless states. That is, we only look at the current state to predict the next word. But when we discuss topics like [self-attention](intro-attention), this relies on the concept of looking at **all** words that come before a given word in a sentence to help predict the next word. The words that are closer in proximity to our word of interest get higher weights.\n",
    "\n",
    "If we were to predict the next word in the sequence based on *both* the current and previous states, we would have a Markov chain of order 2. We know that the $t$-step transition probability is:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(P_t=j | X_0 = i) = \\mathbb{P}(P_{n+t} = j | X_n = i ) = (P^t)_{ij} \\text{ for any }n\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Markov matrix of order 2...\n",
      "\n",
      "mm_order2.shape=(1542, 1542)\n",
      "Selected `community` as the next word to follow `Georgia` with probability 0.218.\n",
      "Georgia students scholarly detailed organization revocation misconduct serious.\n",
      "Serious laboratory even notes unearned honor functions programs.\n",
      "Programs determining provided victim trying part data limited.\n"
     ]
    }
   ],
   "source": [
    "mm_order2 = create_markov_matrix(text, order = 2)\n",
    "print(f'{mm_order2.shape=}')\n",
    "\n",
    "find_next_word(mm_order2, word = 'Georgia')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "build_sents(mm_order2, n_sent = 2, min_words = 8, starter = 'Georgia')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to what we saw in the previous section, these sentences seem to have a bit more creativity. This could be because they take into account more orders of our Markov Chain. Below, we see the probabilities of certain words appearing *after* the current state (`Georgia`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>second_word</th>\n",
       "      <th>georgia_mm_order1</th>\n",
       "      <th>georgia_mm_order2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>institute</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.001744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>policy</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.025079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.015693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>preponderance</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>community</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.218294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>and</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>student</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.054388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>honor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.054054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>academic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.054054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      second_word  georgia_mm_order1  georgia_mm_order2\n",
       "0            tech           0.702703                NaN\n",
       "1       institute           0.216216           0.001744\n",
       "2          policy           0.027027           0.025079\n",
       "3               s           0.027027           0.015693\n",
       "4   preponderance           0.027027                NaN\n",
       "11      community                NaN           0.218294\n",
       "9             and                NaN           0.063423\n",
       "23        student                NaN           0.054388\n",
       "5           honor                NaN           0.054054\n",
       "13       academic                NaN           0.054054"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_georgia = mm.loc['georgia', :]\n",
    "mm2_georgia = mm_order2.loc['georgia', :]\n",
    "\n",
    "pd.merge(mm_georgia[mm_georgia > 0].to_frame().reset_index(),\n",
    "        mm2_georgia[mm2_georgia > 0].to_frame().reset_index(),\n",
    "        how = 'outer',\n",
    "        on = 'index',\n",
    "        suffixes = ('_mm_order1', '_mm_order2')\n",
    ") \\\n",
    "        .rename(columns = {'index': 'second_word'}) \\\n",
    "        .sort_values(['georgia_mm_order1', 'georgia_mm_order2'], ascending = False) \\\n",
    "        .head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the second-order Markov Chain applies a broader brush in determining which words can come after our current state word (`Georgia`). The first-order Markov Chain seems to make the most sense based on these associated probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Based on this methodology, we can create a word-generator. This can help customers who might want unique and creative text generated based on certain findings from the semantic search performed by the language models used in previous sections. The possibilities are limitless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('LLM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b822f9af3a90b72ec1411ef4852a04f57edaf15be20a3869866e07f7e8d4052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
